{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import Phi\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLandau():\n",
    "    def __init__(self, x, gamma, dt):\n",
    "        \n",
    "        self.dt = torch.tensor(dt).float().to(device) # JKO time step\n",
    "        self.gamma = torch.tensor(gamma).float().to(device) # \n",
    "\n",
    "        self.x = x.to(device) # velocity + time\n",
    "        self.nex = x.shape[0] # number of particles\n",
    "        self.d = x.shape[1]-1 # dimension\n",
    "\n",
    "        # neural network\n",
    "        self.net = Phi.Phi(nTh=3, m=20, d=self.d).to(device)\n",
    "\n",
    "        # optimizer Adam\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    def Landau_loss(self, x):\n",
    "        loss = 0\n",
    "        u, grad_u = self.net.Grad_Hess(x, justGrad=False)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        v = x[:,:-1]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            v_diff = v[i,:]-v[:,:] # 2d-matrix\n",
    "            u_diff = u[i,:]-u # 2d-matrix\n",
    "            norm = torch.norm(v_diff, dim=1, keepdim=True)\n",
    "            proj = self.proj(v_diff, norm) # 3d-tensor\n",
    "\n",
    "            # transportation loss\n",
    "            temp = (norm**self.gamma) * torch.squeeze(torch.matmul(u_diff[:,None,:], torch.matmul(proj, u_diff[:,:,None])), dim=1)\n",
    "            transport_cost = torch.sum(temp) / (2*self.nex)\n",
    "\n",
    "            # entropy loss\n",
    "            temp1 = torch.sum(torch.sum(proj * grad_u[i,:,:], dim=1), dim=1)\n",
    "            temp2 = torch.sum(norm**2 * v_diff * u_diff, dim=1)\n",
    "            logdet = torch.dot(torch.squeeze(norm, dim=1)**self.gamma, (temp1 - temp2)) / (self.nex)\n",
    "\n",
    "            loss = loss + (transport_cost - 2*self.dt * logdet) / batch_size\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def proj(self, v_diff, norm):\n",
    "        v_temp = v_diff[:,None,:]\n",
    "        proj = (norm**2)[:,:,None] * torch.eye(self.d).to(device) - v_temp.permute(0,2,1) * v_temp\n",
    "        return proj\n",
    "\n",
    "\n",
    "    # def train(self, batch_size, epoch):\n",
    "    #     for epoch_idx in range(epoch):\n",
    "    #         print(\"Epoch_idx: \", epoch_idx)\n",
    "    #         loader = self.batch(self.x, batch_size)\n",
    "\n",
    "    #         for batch_idx, batch_x in enumerate(loader):\n",
    "    #             iter = 0\n",
    "    #             print(batch_idx)\n",
    "    #             while iter <= 1000:\n",
    "    #                 self.optimizer.zero_grad()\n",
    "    #                 loss = self.Landau_loss(batch_x[0])\n",
    "    #                 if iter % 50 == 0:\n",
    "    #                     print('Batch_idx %d, Iter %d, Loss: %.5e' % (batch_idx, iter, loss.item()))\n",
    "    #                 loss.backward()\n",
    "    #                 self.optimizer.step()\n",
    "    #                 iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        iter = 0\n",
    "        v = self.x[:,:-1]\n",
    "        while iter <= 1000:\n",
    "            u = self.net.Grad_Hess(self.x, justGrad=True)\n",
    "            loss_score = torch.norm(u + v)**2 / self.nex\n",
    "            print('Iter %d, Score Loss: %.5e' % (iter, loss_score.item()))\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.Landau_loss(self.x)\n",
    "            print('Iter %d, Loss: %.5e' % (iter, loss.item()))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "    \n",
    "    def batch(self, x, batch_size):\n",
    "        dataset = torch.utils.data.TensorDataset(x)\n",
    "        loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "        return loader\n",
    "\n",
    "\n",
    "    def compute_det(self, v):\n",
    "        logdet = torch.zeros(self.nex)\n",
    "        u, grad_u = self.net.Grad_Hess(v, justGrad=False)\n",
    "\n",
    "        for i in range(self.nex):\n",
    "            v_diff = v[i,:]-v \n",
    "            u_diff = u[i,:]-u \n",
    "            norm = torch.norm(v_diff, dim=1, keepdim=True)\n",
    "            proj = self.proj(v_diff, norm) \n",
    "            \n",
    "            temp1 = torch.sum(torch.sum(proj * grad_u[i,:,:], dim=1), dim=1)\n",
    "            temp2 = torch.sum(norm**2 * v_diff * u_diff, dim=1)\n",
    "            logdet[i] = torch.dot(torch.squeeze(norm, dim=1)**self.gamma, (temp1 - temp2)) / self.nex\n",
    "        return torch.exp(logdet)\n",
    "\n",
    "\n",
    "    def compute_v(self, v):\n",
    "        v_new = torch.zeros((self.N, self.d))\n",
    "        u = self.net.Grad_Hess(v, justGrad=True)\n",
    "\n",
    "        for i in range(self.nex):\n",
    "            v_diff = v[i,:]-v\n",
    "            norm = torch.norm(v_diff, dim=1, keepdim=True)\n",
    "            proj_i = self.proj(v_diff, norm)\n",
    "\n",
    "            temp = (norm**self.gamma) * torch.squeeze(torch.matmul((u[i,:]-u)[:,None,:], proj_i), dim=1)\n",
    "            v_new[i,:] = v[i,:] - torch.sum(temp, dim=0) / self.nex\n",
    "        return v_new\n",
    "\n",
    "\n",
    "    def compute_f(self,v,f):\n",
    "        det = self.compute_det(self, v)\n",
    "        f_new = f / det\n",
    "        return f_new\n",
    "\n",
    "    \n",
    "    def train_score(self):\n",
    "        iter = 0\n",
    "        v = self.x[:,:-1]\n",
    "        \n",
    "        while iter <= 10000:\n",
    "            self.optimizer.zero_grad()\n",
    "            u = self.net.Grad_Hess(self.x, justGrad=True)\n",
    "            loss = torch.norm(u + v)**2 / self.nex\n",
    "            loss.backward()\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('Iter %d, Loss: %.5e' % (iter, loss.item()))\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss: 8.35033e+00\n",
      "Iter 100, Loss: 2.93457e-02\n",
      "Iter 200, Loss: 1.90268e-02\n",
      "Iter 300, Loss: 1.21458e-02\n",
      "Iter 400, Loss: 8.00944e-03\n",
      "Iter 500, Loss: 5.64439e-03\n",
      "Iter 600, Loss: 4.23359e-03\n",
      "Iter 700, Loss: 3.30305e-03\n",
      "Iter 800, Loss: 2.62168e-03\n",
      "Iter 900, Loss: 2.09120e-03\n",
      "Iter 1000, Loss: 1.66914e-03\n",
      "Iter 1100, Loss: 1.33187e-03\n",
      "Iter 1200, Loss: 1.06284e-03\n",
      "Iter 1300, Loss: 8.49368e-04\n",
      "Iter 1400, Loss: 6.81364e-04\n",
      "Iter 1500, Loss: 5.50592e-04\n",
      "Iter 1600, Loss: 4.50135e-04\n",
      "Iter 1700, Loss: 3.74054e-04\n",
      "Iter 1800, Loss: 3.17192e-04\n",
      "Iter 1900, Loss: 2.75073e-04\n",
      "Iter 2000, Loss: 2.43911e-04\n",
      "Iter 2100, Loss: 2.20610e-04\n",
      "Iter 2200, Loss: 2.02770e-04\n",
      "Iter 2300, Loss: 1.88625e-04\n",
      "Iter 2400, Loss: 1.76950e-04\n",
      "Iter 2500, Loss: 1.66933e-04\n",
      "Iter 2600, Loss: 1.58065e-04\n",
      "Iter 2700, Loss: 1.50030e-04\n",
      "Iter 2800, Loss: 1.42635e-04\n",
      "Iter 2900, Loss: 1.35755e-04\n",
      "Iter 3000, Loss: 1.29308e-04\n",
      "Iter 3100, Loss: 1.23234e-04\n",
      "Iter 3200, Loss: 1.17487e-04\n",
      "Iter 3300, Loss: 1.12034e-04\n",
      "Iter 3400, Loss: 1.06848e-04\n",
      "Iter 3500, Loss: 1.01908e-04\n",
      "Iter 3600, Loss: 9.71994e-05\n",
      "Iter 3700, Loss: 9.27075e-05\n",
      "Iter 3800, Loss: 8.84212e-05\n",
      "Iter 3900, Loss: 8.43308e-05\n",
      "Iter 4000, Loss: 8.04256e-05\n",
      "Iter 4100, Loss: 7.66965e-05\n",
      "Iter 4200, Loss: 7.31320e-05\n",
      "Iter 4300, Loss: 6.97223e-05\n",
      "Iter 4400, Loss: 6.64577e-05\n",
      "Iter 4500, Loss: 6.33272e-05\n",
      "Iter 4600, Loss: 6.03215e-05\n",
      "Iter 4700, Loss: 5.74338e-05\n",
      "Iter 4800, Loss: 5.46565e-05\n",
      "Iter 4900, Loss: 5.19854e-05\n",
      "Iter 5000, Loss: 4.94185e-05\n",
      "Iter 5100, Loss: 4.69554e-05\n",
      "Iter 5200, Loss: 4.45975e-05\n",
      "Iter 5300, Loss: 4.23477e-05\n",
      "Iter 5400, Loss: 4.02099e-05\n",
      "Iter 5500, Loss: 3.81871e-05\n",
      "Iter 5600, Loss: 3.62826e-05\n",
      "Iter 5700, Loss: 3.44969e-05\n",
      "Iter 5800, Loss: 3.28287e-05\n",
      "Iter 5900, Loss: 3.12746e-05\n",
      "Iter 6000, Loss: 2.98283e-05\n",
      "Iter 6100, Loss: 2.84805e-05\n",
      "Iter 6200, Loss: 2.72221e-05\n",
      "Iter 6300, Loss: 2.60419e-05\n",
      "Iter 6400, Loss: 2.49298e-05\n",
      "Iter 6500, Loss: 2.38758e-05\n",
      "Iter 6600, Loss: 2.28709e-05\n",
      "Iter 6700, Loss: 2.19077e-05\n",
      "Iter 6800, Loss: 2.09801e-05\n",
      "Iter 6900, Loss: 2.00826e-05\n",
      "Iter 7000, Loss: 1.92116e-05\n",
      "Iter 7100, Loss: 1.83640e-05\n",
      "Iter 7200, Loss: 1.75371e-05\n",
      "Iter 7300, Loss: 1.67296e-05\n",
      "Iter 7400, Loss: 1.59403e-05\n",
      "Iter 7500, Loss: 1.51682e-05\n",
      "Iter 7600, Loss: 1.44132e-05\n",
      "Iter 7700, Loss: 1.36753e-05\n",
      "Iter 7800, Loss: 1.29545e-05\n",
      "Iter 7900, Loss: 1.22510e-05\n",
      "Iter 8000, Loss: 1.15887e-05\n",
      "Iter 8100, Loss: 1.09516e-05\n",
      "Iter 8200, Loss: 1.05047e-05\n",
      "Iter 8300, Loss: 9.77553e-06\n",
      "Iter 8400, Loss: 9.28456e-06\n",
      "Iter 8500, Loss: 8.72149e-06\n",
      "Iter 8600, Loss: 8.27479e-06\n",
      "Iter 8700, Loss: 7.79378e-06\n",
      "Iter 8800, Loss: 8.91442e-06\n",
      "Iter 8900, Loss: 6.97245e-06\n",
      "Iter 9000, Loss: 6.59358e-06\n",
      "Iter 9100, Loss: 6.86750e-06\n",
      "Iter 9200, Loss: 5.91571e-06\n",
      "Iter 9300, Loss: 5.60552e-06\n",
      "Iter 9400, Loss: 5.32484e-06\n",
      "Iter 9500, Loss: 5.04141e-06\n",
      "Iter 9600, Loss: 2.41161e-05\n",
      "Iter 9700, Loss: 4.54909e-06\n",
      "Iter 9800, Loss: 4.31998e-06\n",
      "Iter 9900, Loss: 4.40626e-06\n",
      "Iter 10000, Loss: 3.91746e-06\n"
     ]
    }
   ],
   "source": [
    "nex = 100\n",
    "d = 2\n",
    "\n",
    "x = torch.zeros(nex,d+1)\n",
    "v = torch.randn(nex,d)\n",
    "x[:,:d] = v\n",
    "\n",
    "model = DeepLandau(x, gamma=0, dt=1e-3)\n",
    "model.train_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Score Loss: 3.91562e-06\n",
      "Iter 0, Loss: -4.70257e-02\n",
      "Iter 1, Score Loss: 3.40750e-03\n",
      "Iter 1, Loss: -4.88087e-02\n",
      "Iter 2, Score Loss: 2.26070e-02\n",
      "Iter 2, Loss: -5.01600e-02\n",
      "Iter 3, Score Loss: 8.76243e-02\n",
      "Iter 3, Loss: -2.16747e-02\n",
      "Iter 4, Score Loss: 9.22067e-02\n",
      "Iter 4, Loss: -4.82626e-02\n",
      "Iter 5, Score Loss: 1.27343e-01\n",
      "Iter 5, Loss: -3.44796e-02\n",
      "Iter 6, Score Loss: 1.89890e-01\n",
      "Iter 6, Loss: -6.08837e-02\n",
      "Iter 7, Score Loss: 2.88950e-01\n",
      "Iter 7, Loss: -4.16734e-02\n",
      "Iter 8, Score Loss: 3.52405e-01\n",
      "Iter 8, Loss: -6.02099e-02\n",
      "Iter 9, Score Loss: 4.04511e-01\n",
      "Iter 9, Loss: -6.12354e-02\n",
      "Iter 10, Score Loss: 4.91038e-01\n",
      "Iter 10, Loss: -5.48132e-02\n",
      "Iter 11, Score Loss: 6.28527e-01\n",
      "Iter 11, Loss: -7.14050e-02\n",
      "Iter 12, Score Loss: 8.10497e-01\n",
      "Iter 12, Loss: -6.61798e-02\n",
      "Iter 13, Score Loss: 9.71954e-01\n",
      "Iter 13, Loss: -6.79173e-02\n",
      "Iter 14, Score Loss: 1.10237e+00\n",
      "Iter 14, Loss: -7.92622e-02\n",
      "Iter 15, Score Loss: 1.26485e+00\n",
      "Iter 15, Loss: -7.37631e-02\n",
      "Iter 16, Score Loss: 1.49070e+00\n",
      "Iter 16, Loss: -7.92960e-02\n",
      "Iter 17, Score Loss: 1.77115e+00\n",
      "Iter 17, Loss: -8.65160e-02\n",
      "Iter 18, Score Loss: 2.06672e+00\n",
      "Iter 18, Loss: -8.27672e-02\n",
      "Iter 19, Score Loss: 2.33234e+00\n",
      "Iter 19, Loss: -9.03195e-02\n",
      "Iter 20, Score Loss: 2.59605e+00\n",
      "Iter 20, Loss: -9.34445e-02\n",
      "Iter 21, Score Loss: 2.90722e+00\n",
      "Iter 21, Loss: -9.24772e-02\n",
      "Iter 22, Score Loss: 3.28531e+00\n",
      "Iter 22, Loss: -1.00337e-01\n",
      "Iter 23, Score Loss: 3.71504e+00\n",
      "Iter 23, Loss: -1.00442e-01\n",
      "Iter 24, Score Loss: 4.12409e+00\n",
      "Iter 24, Loss: -1.03494e-01\n",
      "Iter 25, Score Loss: 4.49938e+00\n",
      "Iter 25, Loss: -1.09001e-01\n",
      "Iter 26, Score Loss: 4.92414e+00\n",
      "Iter 26, Loss: -1.08559e-01\n",
      "Iter 27, Score Loss: 5.43769e+00\n",
      "Iter 27, Loss: -1.14757e-01\n",
      "Iter 28, Score Loss: 6.00311e+00\n",
      "Iter 28, Loss: -1.16233e-01\n",
      "Iter 29, Score Loss: 6.54343e+00\n",
      "Iter 29, Loss: -1.19600e-01\n",
      "Iter 30, Score Loss: 7.06458e+00\n",
      "Iter 30, Loss: -1.23955e-01\n",
      "Iter 31, Score Loss: 7.64310e+00\n",
      "Iter 31, Loss: -1.25323e-01\n",
      "Iter 32, Score Loss: 8.31540e+00\n",
      "Iter 32, Loss: -1.30862e-01\n",
      "Iter 33, Score Loss: 9.03007e+00\n",
      "Iter 33, Loss: -1.32147e-01\n",
      "Iter 34, Score Loss: 9.68635e+00\n",
      "Iter 34, Loss: -1.37407e-01\n",
      "Iter 35, Score Loss: 1.03587e+01\n",
      "Iter 35, Loss: -1.39426e-01\n",
      "Iter 36, Score Loss: 1.11351e+01\n",
      "Iter 36, Loss: -1.44094e-01\n",
      "Iter 37, Score Loss: 1.19788e+01\n",
      "Iter 37, Loss: -1.46868e-01\n",
      "Iter 38, Score Loss: 1.27905e+01\n",
      "Iter 38, Loss: -1.51186e-01\n",
      "Iter 39, Score Loss: 1.35803e+01\n",
      "Iter 39, Loss: -1.54366e-01\n",
      "Iter 40, Score Loss: 1.44873e+01\n",
      "Iter 40, Loss: -1.58689e-01\n",
      "Iter 41, Score Loss: 1.54794e+01\n",
      "Iter 41, Loss: -1.62014e-01\n",
      "Iter 42, Score Loss: 1.64358e+01\n",
      "Iter 42, Loss: -1.66596e-01\n",
      "Iter 43, Score Loss: 1.73926e+01\n",
      "Iter 43, Loss: -1.69894e-01\n",
      "Iter 44, Score Loss: 1.84999e+01\n",
      "Iter 44, Loss: -1.74828e-01\n",
      "Iter 45, Score Loss: 1.96705e+01\n",
      "Iter 45, Loss: -1.78157e-01\n",
      "Iter 46, Score Loss: 2.08208e+01\n",
      "Iter 46, Loss: -1.83101e-01\n",
      "Iter 47, Score Loss: 2.19723e+01\n",
      "Iter 47, Loss: -1.85986e-01\n",
      "Iter 48, Score Loss: 2.33493e+01\n",
      "Iter 48, Loss: -1.88372e-01\n",
      "Iter 49, Score Loss: 2.44045e+01\n",
      "Iter 49, Loss: -1.88616e-01\n",
      "Iter 50, Score Loss: 2.55121e+01\n",
      "Iter 50, Loss: -1.94980e-01\n",
      "Iter 51, Score Loss: 2.66464e+01\n",
      "Iter 51, Loss: -2.03780e-01\n",
      "Iter 52, Score Loss: 2.78118e+01\n",
      "Iter 52, Loss: -2.02654e-01\n",
      "Iter 53, Score Loss: 2.89870e+01\n",
      "Iter 53, Loss: -2.07841e-01\n",
      "Iter 54, Score Loss: 3.02107e+01\n",
      "Iter 54, Loss: -2.15499e-01\n",
      "Iter 55, Score Loss: 3.15484e+01\n",
      "Iter 55, Loss: -2.15250e-01\n",
      "Iter 56, Score Loss: 3.28951e+01\n",
      "Iter 56, Loss: -2.21179e-01\n",
      "Iter 57, Score Loss: 3.42641e+01\n",
      "Iter 57, Loss: -2.27265e-01\n",
      "Iter 58, Score Loss: 3.57625e+01\n",
      "Iter 58, Loss: -2.28148e-01\n",
      "Iter 59, Score Loss: 3.72998e+01\n",
      "Iter 59, Loss: -2.34256e-01\n",
      "Iter 60, Score Loss: 3.88527e+01\n",
      "Iter 60, Loss: -2.39678e-01\n",
      "Iter 61, Score Loss: 4.05014e+01\n",
      "Iter 61, Loss: -2.41290e-01\n",
      "Iter 62, Score Loss: 4.22095e+01\n",
      "Iter 62, Loss: -2.47343e-01\n",
      "Iter 63, Score Loss: 4.39687e+01\n",
      "Iter 63, Loss: -2.52658e-01\n",
      "Iter 64, Score Loss: 4.57832e+01\n",
      "Iter 64, Loss: -2.55002e-01\n",
      "Iter 65, Score Loss: 4.76639e+01\n",
      "Iter 65, Loss: -2.60408e-01\n",
      "Iter 66, Score Loss: 4.96533e+01\n",
      "Iter 66, Loss: -2.66147e-01\n",
      "Iter 67, Score Loss: 5.16616e+01\n",
      "Iter 67, Loss: -2.69626e-01\n",
      "Iter 68, Score Loss: 5.37601e+01\n",
      "Iter 68, Loss: -2.73832e-01\n",
      "Iter 69, Score Loss: 5.60179e+01\n",
      "Iter 69, Loss: -2.79504e-01\n",
      "Iter 70, Score Loss: 5.82335e+01\n",
      "Iter 70, Loss: -2.84914e-01\n",
      "Iter 71, Score Loss: 6.06227e+01\n",
      "Iter 71, Loss: -2.89158e-01\n",
      "Iter 72, Score Loss: 6.31747e+01\n",
      "Iter 72, Loss: -2.93242e-01\n",
      "Iter 73, Score Loss: 6.56304e+01\n",
      "Iter 73, Loss: -2.98621e-01\n",
      "Iter 74, Score Loss: 6.83658e+01\n",
      "Iter 74, Loss: -3.04793e-01\n",
      "Iter 75, Score Loss: 7.11227e+01\n",
      "Iter 75, Loss: -3.10316e-01\n",
      "Iter 76, Score Loss: 7.39436e+01\n",
      "Iter 76, Loss: -3.15090e-01\n",
      "Iter 77, Score Loss: 7.70395e+01\n",
      "Iter 77, Loss: -3.19757e-01\n",
      "Iter 78, Score Loss: 8.00222e+01\n",
      "Iter 78, Loss: -3.24367e-01\n",
      "Iter 79, Score Loss: 8.32775e+01\n",
      "Iter 79, Loss: -3.28451e-01\n",
      "Iter 80, Score Loss: 8.64244e+01\n",
      "Iter 80, Loss: -3.31907e-01\n",
      "Iter 81, Score Loss: 8.96981e+01\n",
      "Iter 81, Loss: -3.36081e-01\n",
      "Iter 82, Score Loss: 9.28023e+01\n",
      "Iter 82, Loss: -3.42698e-01\n",
      "Iter 83, Score Loss: 9.61824e+01\n",
      "Iter 83, Loss: -3.50742e-01\n",
      "Iter 84, Score Loss: 9.93241e+01\n",
      "Iter 84, Loss: -3.57615e-01\n",
      "Iter 85, Score Loss: 1.02902e+02\n",
      "Iter 85, Loss: -3.63193e-01\n",
      "Iter 86, Score Loss: 1.06392e+02\n",
      "Iter 86, Loss: -3.68156e-01\n",
      "Iter 87, Score Loss: 1.09948e+02\n",
      "Iter 87, Loss: -3.72252e-01\n",
      "Iter 88, Score Loss: 1.13932e+02\n",
      "Iter 88, Loss: -3.75440e-01\n",
      "Iter 89, Score Loss: 1.17371e+02\n",
      "Iter 89, Loss: -3.80164e-01\n",
      "Iter 90, Score Loss: 1.21527e+02\n",
      "Iter 90, Loss: -3.88354e-01\n",
      "Iter 91, Score Loss: 1.25273e+02\n",
      "Iter 91, Loss: -3.96841e-01\n",
      "Iter 92, Score Loss: 1.29260e+02\n",
      "Iter 92, Loss: -4.02181e-01\n",
      "Iter 93, Score Loss: 1.33649e+02\n",
      "Iter 93, Loss: -4.06895e-01\n",
      "Iter 94, Score Loss: 1.37640e+02\n",
      "Iter 94, Loss: -4.13321e-01\n",
      "Iter 95, Score Loss: 1.42214e+02\n",
      "Iter 95, Loss: -4.19566e-01\n",
      "Iter 96, Score Loss: 1.46666e+02\n",
      "Iter 96, Loss: -4.23250e-01\n",
      "Iter 97, Score Loss: 1.51192e+02\n",
      "Iter 97, Loss: -4.25417e-01\n",
      "Iter 98, Score Loss: 1.55672e+02\n",
      "Iter 98, Loss: -4.30354e-01\n",
      "Iter 99, Score Loss: 1.60340e+02\n",
      "Iter 99, Loss: -4.37242e-01\n",
      "Iter 100, Score Loss: 1.64437e+02\n",
      "Iter 100, Loss: -4.41760e-01\n",
      "Iter 101, Score Loss: 1.69471e+02\n",
      "Iter 101, Loss: -4.48364e-01\n",
      "Iter 102, Score Loss: 1.73609e+02\n",
      "Iter 102, Loss: -4.60525e-01\n",
      "Iter 103, Score Loss: 1.78342e+02\n",
      "Iter 103, Loss: -4.67250e-01\n",
      "Iter 104, Score Loss: 1.83515e+02\n",
      "Iter 104, Loss: -4.68644e-01\n",
      "Iter 105, Score Loss: 1.88005e+02\n",
      "Iter 105, Loss: -4.74408e-01\n",
      "Iter 106, Score Loss: 1.93412e+02\n",
      "Iter 106, Loss: -4.81508e-01\n",
      "Iter 107, Score Loss: 1.98467e+02\n",
      "Iter 107, Loss: -4.85191e-01\n",
      "Iter 108, Score Loss: 2.03628e+02\n",
      "Iter 108, Loss: -4.92306e-01\n",
      "Iter 109, Score Loss: 2.08880e+02\n",
      "Iter 109, Loss: -5.02918e-01\n",
      "Iter 110, Score Loss: 2.14467e+02\n",
      "Iter 110, Loss: -5.08096e-01\n",
      "Iter 111, Score Loss: 2.19859e+02\n",
      "Iter 111, Loss: -5.12511e-01\n",
      "Iter 112, Score Loss: 2.25637e+02\n",
      "Iter 112, Loss: -5.20640e-01\n",
      "Iter 113, Score Loss: 2.31668e+02\n",
      "Iter 113, Loss: -5.26086e-01\n",
      "Iter 114, Score Loss: 2.37265e+02\n",
      "Iter 114, Loss: -5.28597e-01\n",
      "Iter 115, Score Loss: 2.43910e+02\n",
      "Iter 115, Loss: -5.33282e-01\n",
      "Iter 116, Score Loss: 2.49495e+02\n",
      "Iter 116, Loss: -5.37867e-01\n",
      "Iter 117, Score Loss: 2.56134e+02\n",
      "Iter 117, Loss: -5.38834e-01\n",
      "Iter 118, Score Loss: 2.61824e+02\n",
      "Iter 118, Loss: -5.44351e-01\n",
      "Iter 119, Score Loss: 2.68254e+02\n",
      "Iter 119, Loss: -5.58378e-01\n",
      "Iter 120, Score Loss: 2.74008e+02\n",
      "Iter 120, Loss: -5.68535e-01\n",
      "Iter 121, Score Loss: 2.80602e+02\n",
      "Iter 121, Loss: -5.74228e-01\n",
      "Iter 122, Score Loss: 2.87184e+02\n",
      "Iter 122, Loss: -5.79777e-01\n",
      "Iter 123, Score Loss: 2.93407e+02\n",
      "Iter 123, Loss: -5.81780e-01\n",
      "Iter 124, Score Loss: 3.00851e+02\n",
      "Iter 124, Loss: -5.85511e-01\n",
      "Iter 125, Score Loss: 3.07008e+02\n",
      "Iter 125, Loss: -5.97258e-01\n",
      "Iter 126, Score Loss: 3.14220e+02\n",
      "Iter 126, Loss: -6.06989e-01\n",
      "Iter 127, Score Loss: 3.21476e+02\n",
      "Iter 127, Loss: -6.11444e-01\n",
      "Iter 128, Score Loss: 3.28345e+02\n",
      "Iter 128, Loss: -6.17263e-01\n",
      "Iter 129, Score Loss: 3.36181e+02\n",
      "Iter 129, Loss: -6.22659e-01\n",
      "Iter 130, Score Loss: 3.43403e+02\n",
      "Iter 130, Loss: -6.25970e-01\n",
      "Iter 131, Score Loss: 3.51256e+02\n",
      "Iter 131, Loss: -6.32947e-01\n",
      "Iter 132, Score Loss: 3.58669e+02\n",
      "Iter 132, Loss: -6.43318e-01\n",
      "Iter 133, Score Loss: 3.66844e+02\n",
      "Iter 133, Loss: -6.50672e-01\n",
      "Iter 134, Score Loss: 3.74589e+02\n",
      "Iter 134, Loss: -6.56756e-01\n",
      "Iter 135, Score Loss: 3.82946e+02\n",
      "Iter 135, Loss: -6.64766e-01\n",
      "Iter 136, Score Loss: 3.91456e+02\n",
      "Iter 136, Loss: -6.71493e-01\n",
      "Iter 137, Score Loss: 3.99706e+02\n",
      "Iter 137, Loss: -6.75505e-01\n",
      "Iter 138, Score Loss: 4.09016e+02\n",
      "Iter 138, Loss: -6.79514e-01\n",
      "Iter 139, Score Loss: 4.17162e+02\n",
      "Iter 139, Loss: -6.84106e-01\n",
      "Iter 140, Score Loss: 4.26804e+02\n",
      "Iter 140, Loss: -6.86175e-01\n",
      "Iter 141, Score Loss: 4.34890e+02\n",
      "Iter 141, Loss: -6.85682e-01\n",
      "Iter 142, Score Loss: 4.44291e+02\n",
      "Iter 142, Loss: -6.91519e-01\n",
      "Iter 143, Score Loss: 4.52114e+02\n",
      "Iter 143, Loss: -7.08067e-01\n",
      "Iter 144, Score Loss: 4.61247e+02\n",
      "Iter 144, Loss: -7.23155e-01\n",
      "Iter 145, Score Loss: 4.69781e+02\n",
      "Iter 145, Loss: -7.29050e-01\n",
      "Iter 146, Score Loss: 4.78562e+02\n",
      "Iter 146, Loss: -7.31749e-01\n",
      "Iter 147, Score Loss: 4.88193e+02\n",
      "Iter 147, Loss: -7.34913e-01\n",
      "Iter 148, Score Loss: 4.96479e+02\n",
      "Iter 148, Loss: -7.40758e-01\n",
      "Iter 149, Score Loss: 5.06409e+02\n",
      "Iter 149, Loss: -7.52945e-01\n",
      "Iter 150, Score Loss: 5.15320e+02\n",
      "Iter 150, Loss: -7.65625e-01\n",
      "Iter 151, Score Loss: 5.24686e+02\n",
      "Iter 151, Loss: -7.69840e-01\n",
      "Iter 152, Score Loss: 5.35064e+02\n",
      "Iter 152, Loss: -7.70838e-01\n",
      "Iter 153, Score Loss: 5.44150e+02\n",
      "Iter 153, Loss: -7.78063e-01\n",
      "Iter 154, Score Loss: 5.54777e+02\n",
      "Iter 154, Loss: -7.89301e-01\n",
      "Iter 155, Score Loss: 5.64776e+02\n",
      "Iter 155, Loss: -7.97787e-01\n",
      "Iter 156, Score Loss: 5.75042e+02\n",
      "Iter 156, Loss: -8.04609e-01\n",
      "Iter 157, Score Loss: 5.86005e+02\n",
      "Iter 157, Loss: -8.12717e-01\n",
      "Iter 158, Score Loss: 5.96480e+02\n",
      "Iter 158, Loss: -8.19612e-01\n",
      "Iter 159, Score Loss: 6.07820e+02\n",
      "Iter 159, Loss: -8.23915e-01\n",
      "Iter 160, Score Loss: 6.18792e+02\n",
      "Iter 160, Loss: -8.29148e-01\n",
      "Iter 161, Score Loss: 6.30300e+02\n",
      "Iter 161, Loss: -8.38065e-01\n",
      "Iter 162, Score Loss: 6.41391e+02\n",
      "Iter 162, Loss: -8.48229e-01\n",
      "Iter 163, Score Loss: 6.53450e+02\n",
      "Iter 163, Loss: -8.56110e-01\n",
      "Iter 164, Score Loss: 6.64823e+02\n",
      "Iter 164, Loss: -8.63029e-01\n",
      "Iter 165, Score Loss: 6.77360e+02\n",
      "Iter 165, Loss: -8.71816e-01\n",
      "Iter 166, Score Loss: 6.89311e+02\n",
      "Iter 166, Loss: -8.81921e-01\n",
      "Iter 167, Score Loss: 7.02060e+02\n",
      "Iter 167, Loss: -8.90812e-01\n",
      "Iter 168, Score Loss: 7.14904e+02\n",
      "Iter 168, Loss: -8.97983e-01\n",
      "Iter 169, Score Loss: 7.27941e+02\n",
      "Iter 169, Loss: -9.04984e-01\n",
      "Iter 170, Score Loss: 7.41398e+02\n",
      "Iter 170, Loss: -9.13204e-01\n",
      "Iter 171, Score Loss: 7.54939e+02\n",
      "Iter 171, Loss: -9.22388e-01\n",
      "Iter 172, Score Loss: 7.68745e+02\n",
      "Iter 172, Loss: -9.31367e-01\n",
      "Iter 173, Score Loss: 7.83162e+02\n",
      "Iter 173, Loss: -9.38428e-01\n",
      "Iter 174, Score Loss: 7.97124e+02\n",
      "Iter 174, Loss: -9.40262e-01\n",
      "Iter 175, Score Loss: 8.12665e+02\n",
      "Iter 175, Loss: -9.27280e-01\n",
      "Iter 176, Score Loss: 8.25336e+02\n",
      "Iter 176, Loss: -8.86024e-01\n",
      "Iter 177, Score Loss: 8.40698e+02\n",
      "Iter 177, Loss: -8.56595e-01\n",
      "Iter 178, Score Loss: 8.50837e+02\n",
      "Iter 178, Loss: -9.28814e-01\n",
      "Iter 179, Score Loss: 8.62213e+02\n",
      "Iter 179, Loss: -9.76359e-01\n",
      "Iter 180, Score Loss: 8.75314e+02\n",
      "Iter 180, Loss: -9.31733e-01\n",
      "Iter 181, Score Loss: 8.85370e+02\n",
      "Iter 181, Loss: -9.63614e-01\n",
      "Iter 182, Score Loss: 8.96576e+02\n",
      "Iter 182, Loss: -9.94803e-01\n",
      "Iter 183, Score Loss: 9.09475e+02\n",
      "Iter 183, Loss: -9.71498e-01\n",
      "Iter 184, Score Loss: 9.20142e+02\n",
      "Iter 184, Loss: -1.00299e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 79\u001b[0m, in \u001b[0;36mDeepLandau.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLandau_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m%.5e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28miter\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m---> 79\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
