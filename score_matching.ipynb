{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, d, m, nTh, nex):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if nTh < 2:\n",
    "            print(\"nTh must be an integer >= 2\")\n",
    "            exit(1)\n",
    "\n",
    "        self.d = d\n",
    "        self.m = m\n",
    "        self.nTh = nTh\n",
    "        self.nex = nex\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([])\n",
    "        self.layers.append(torch.nn.Linear(d, m, bias=True))\n",
    "        for i in range(nTh-2):\n",
    "            self.layers.append(torch.nn.Linear(m, m, bias=True))\n",
    "        self.layers.append(torch.nn.Linear(m, d, bias=False))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layers[0].forward(x))\n",
    "        for i in range(1, self.nTh-1):\n",
    "            x = torch.relu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score_Matching():\n",
    "    def __init__(self, v):\n",
    "        self.v = v.to(device) # velocity\n",
    "        self.nex = v.shape[0] # number of particles\n",
    "        self.d = v.shape[1] # dimension\n",
    "\n",
    "        # neural network\n",
    "        self.net = DNN(nTh=4, m=100, d=self.d, nex=self.nex).to(device)\n",
    "\n",
    "        # optimizer Adam\n",
    "        self.optimizer = torch.optim.ASGD(self.net.parameters(), lr=1e-2)\n",
    "\n",
    "        # self.optimizer2 = torch.optim.LBFGS(\n",
    "        #     self.net.parameters(), \n",
    "        #     lr=1.0, \n",
    "        #     max_iter=10000, \n",
    "        #     max_eval=50000, \n",
    "        #     history_size=100,\n",
    "        #     tolerance_grad=1e-16, \n",
    "        #     tolerance_change=1.0 * np.finfo(float).eps,\n",
    "        #     line_search_fn=\"strong_wolfe\")\n",
    "        self.iter = 0\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        v = self.v\n",
    "\n",
    "        div_u = 0\n",
    "        u = self.net(v)\n",
    "        for i in range(self.nex):\n",
    "            u_v = torch.autograd.functional.jacobian(self.net, v[i,:], create_graph=True)\n",
    "            div_u = div_u + u_v.trace()\n",
    "\n",
    "        loss = (torch.norm(u)**2 + 2*div_u ) / self.nex\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        print('Iter %d, Loss: %.5e' % (self.iter, loss.item()))\n",
    "\n",
    "        self.iter += 1\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # def loss(self):\n",
    "    #     v=self.v\n",
    "    #     loss = torch.sum( torch.pow(self.net(v) + v, 2) * torch.exp(-1* v**2 / 2) ) / self.nex\n",
    "\n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "\n",
    "    #     print('Iter %d, Loss: %.5e' % (self.iter, loss.item()))\n",
    "\n",
    "    #     self.iter += 1\n",
    "    #     return loss\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        while self.iter <= 2000:\n",
    "            self.optimizer.step(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        u = self.net(self.v)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nex = 500\n",
    "d = 1\n",
    "v = torch.randn(nex, d, requires_grad=True)\n",
    "\n",
    "model = Score_Matching(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9, Loss: -2.37968e-01\n",
      "Iter 10, Loss: -2.60621e-01\n",
      "Iter 11, Loss: -2.83367e-01\n",
      "Iter 12, Loss: -3.06170e-01\n",
      "Iter 13, Loss: -3.28603e-01\n",
      "Iter 14, Loss: -3.51560e-01\n",
      "Iter 15, Loss: -3.74878e-01\n",
      "Iter 16, Loss: -3.97809e-01\n",
      "Iter 17, Loss: -4.21678e-01\n",
      "Iter 18, Loss: -4.44537e-01\n",
      "Iter 19, Loss: -4.68233e-01\n",
      "Iter 20, Loss: -4.91919e-01\n",
      "Iter 21, Loss: -5.14178e-01\n",
      "Iter 22, Loss: -5.35519e-01\n",
      "Iter 23, Loss: -5.57774e-01\n",
      "Iter 24, Loss: -5.77073e-01\n",
      "Iter 25, Loss: -5.97901e-01\n",
      "Iter 26, Loss: -6.19350e-01\n",
      "Iter 27, Loss: -6.37530e-01\n",
      "Iter 28, Loss: -6.57574e-01\n",
      "Iter 29, Loss: -6.76293e-01\n",
      "Iter 30, Loss: -6.93233e-01\n",
      "Iter 31, Loss: -7.09717e-01\n",
      "Iter 32, Loss: -7.24693e-01\n",
      "Iter 33, Loss: -7.38048e-01\n",
      "Iter 34, Loss: -7.53416e-01\n",
      "Iter 35, Loss: -7.67985e-01\n",
      "Iter 36, Loss: -7.77464e-01\n",
      "Iter 37, Loss: -7.87927e-01\n",
      "Iter 38, Loss: -7.98609e-01\n",
      "Iter 39, Loss: -8.11597e-01\n",
      "Iter 40, Loss: -8.19885e-01\n",
      "Iter 41, Loss: -8.29027e-01\n",
      "Iter 42, Loss: -8.34505e-01\n",
      "Iter 43, Loss: -8.40703e-01\n",
      "Iter 44, Loss: -8.49079e-01\n",
      "Iter 45, Loss: -8.52952e-01\n",
      "Iter 46, Loss: -8.59161e-01\n",
      "Iter 47, Loss: -8.65220e-01\n",
      "Iter 48, Loss: -8.67535e-01\n",
      "Iter 49, Loss: -8.72309e-01\n",
      "Iter 50, Loss: -8.78725e-01\n",
      "Iter 51, Loss: -8.80770e-01\n",
      "Iter 52, Loss: -8.86107e-01\n",
      "Iter 53, Loss: -8.87841e-01\n",
      "Iter 54, Loss: -8.87913e-01\n",
      "Iter 55, Loss: -8.87932e-01\n",
      "Iter 56, Loss: -8.83668e-01\n",
      "Iter 57, Loss: -8.87966e-01\n",
      "Iter 58, Loss: -8.91141e-01\n",
      "Iter 59, Loss: -8.92043e-01\n",
      "Iter 60, Loss: -8.95354e-01\n",
      "Iter 61, Loss: -8.95023e-01\n",
      "Iter 62, Loss: -8.97987e-01\n",
      "Iter 63, Loss: -8.99416e-01\n",
      "Iter 64, Loss: -8.98767e-01\n",
      "Iter 65, Loss: -8.95931e-01\n",
      "Iter 66, Loss: -8.96233e-01\n",
      "Iter 67, Loss: -8.92754e-01\n",
      "Iter 68, Loss: -8.90693e-01\n",
      "Iter 69, Loss: -8.88548e-01\n",
      "Iter 70, Loss: -8.90087e-01\n",
      "Iter 71, Loss: -8.88373e-01\n",
      "Iter 72, Loss: -8.83245e-01\n",
      "Iter 73, Loss: -8.82346e-01\n",
      "Iter 74, Loss: -8.80244e-01\n",
      "Iter 75, Loss: -8.77084e-01\n",
      "Iter 76, Loss: -8.75899e-01\n",
      "Iter 77, Loss: -8.70362e-01\n",
      "Iter 78, Loss: -8.68230e-01\n",
      "Iter 79, Loss: -8.67325e-01\n",
      "Iter 80, Loss: -8.65739e-01\n",
      "Iter 81, Loss: -8.61332e-01\n",
      "Iter 82, Loss: -8.57758e-01\n",
      "Iter 83, Loss: -8.56782e-01\n",
      "Iter 84, Loss: -8.56703e-01\n",
      "Iter 85, Loss: -8.54583e-01\n",
      "Iter 86, Loss: -8.55436e-01\n",
      "Iter 87, Loss: -8.52845e-01\n",
      "Iter 88, Loss: -8.49657e-01\n",
      "Iter 89, Loss: -8.48731e-01\n",
      "Iter 90, Loss: -8.45862e-01\n",
      "Iter 91, Loss: -8.40952e-01\n",
      "Iter 92, Loss: -8.41552e-01\n",
      "Iter 93, Loss: -8.38406e-01\n",
      "Iter 94, Loss: -8.36289e-01\n",
      "Iter 95, Loss: -8.31999e-01\n",
      "Iter 96, Loss: -8.29389e-01\n",
      "Iter 97, Loss: -8.27229e-01\n",
      "Iter 98, Loss: -8.24811e-01\n",
      "Iter 99, Loss: -8.25885e-01\n",
      "Iter 100, Loss: -8.25639e-01\n",
      "Iter 101, Loss: -8.20127e-01\n",
      "Iter 102, Loss: -8.16502e-01\n",
      "Iter 103, Loss: -8.14303e-01\n",
      "Iter 104, Loss: -8.09726e-01\n",
      "Iter 105, Loss: -8.04137e-01\n",
      "Iter 106, Loss: -8.01356e-01\n",
      "Iter 107, Loss: -8.01196e-01\n",
      "Iter 108, Loss: -7.96724e-01\n",
      "Iter 109, Loss: -7.97393e-01\n",
      "Iter 110, Loss: -7.98150e-01\n",
      "Iter 111, Loss: -7.93426e-01\n",
      "Iter 112, Loss: -7.93359e-01\n",
      "Iter 113, Loss: -7.87143e-01\n",
      "Iter 114, Loss: -7.83981e-01\n",
      "Iter 115, Loss: -7.82428e-01\n",
      "Iter 116, Loss: -7.84373e-01\n",
      "Iter 117, Loss: -7.80502e-01\n",
      "Iter 118, Loss: -7.82964e-01\n",
      "Iter 119, Loss: -7.79936e-01\n",
      "Iter 120, Loss: -7.76281e-01\n",
      "Iter 121, Loss: -7.78418e-01\n",
      "Iter 122, Loss: -7.74127e-01\n",
      "Iter 123, Loss: -7.72390e-01\n",
      "Iter 124, Loss: -7.70226e-01\n",
      "Iter 125, Loss: -7.68058e-01\n",
      "Iter 126, Loss: -7.62432e-01\n",
      "Iter 127, Loss: -7.59137e-01\n",
      "Iter 128, Loss: -7.60145e-01\n",
      "Iter 129, Loss: -7.58981e-01\n",
      "Iter 130, Loss: -7.59602e-01\n",
      "Iter 131, Loss: -7.57129e-01\n",
      "Iter 132, Loss: -7.50581e-01\n",
      "Iter 133, Loss: -7.49424e-01\n",
      "Iter 134, Loss: -7.49278e-01\n",
      "Iter 135, Loss: -7.50429e-01\n",
      "Iter 136, Loss: -7.53500e-01\n",
      "Iter 137, Loss: -7.48767e-01\n",
      "Iter 138, Loss: -7.44982e-01\n",
      "Iter 139, Loss: -7.39340e-01\n",
      "Iter 140, Loss: -7.38548e-01\n",
      "Iter 141, Loss: -7.38218e-01\n",
      "Iter 142, Loss: -7.37610e-01\n",
      "Iter 143, Loss: -7.38809e-01\n",
      "Iter 144, Loss: -7.39837e-01\n",
      "Iter 145, Loss: -7.39103e-01\n",
      "Iter 146, Loss: -7.39885e-01\n",
      "Iter 147, Loss: -7.39689e-01\n",
      "Iter 148, Loss: -7.39462e-01\n",
      "Iter 149, Loss: -7.35255e-01\n",
      "Iter 150, Loss: -7.35005e-01\n",
      "Iter 151, Loss: -7.36958e-01\n",
      "Iter 152, Loss: -7.36555e-01\n",
      "Iter 153, Loss: -7.34795e-01\n",
      "Iter 154, Loss: -7.35146e-01\n",
      "Iter 155, Loss: -7.34685e-01\n",
      "Iter 156, Loss: -7.31726e-01\n",
      "Iter 157, Loss: -7.33574e-01\n",
      "Iter 158, Loss: -7.29766e-01\n",
      "Iter 159, Loss: -7.32465e-01\n",
      "Iter 160, Loss: -7.33512e-01\n",
      "Iter 161, Loss: -7.31442e-01\n",
      "Iter 162, Loss: -7.30636e-01\n",
      "Iter 163, Loss: -7.29818e-01\n",
      "Iter 164, Loss: -7.32630e-01\n",
      "Iter 165, Loss: -7.29859e-01\n",
      "Iter 166, Loss: -7.30969e-01\n",
      "Iter 167, Loss: -7.29430e-01\n",
      "Iter 168, Loss: -7.31346e-01\n",
      "Iter 169, Loss: -7.30842e-01\n",
      "Iter 170, Loss: -7.32921e-01\n",
      "Iter 171, Loss: -7.30134e-01\n",
      "Iter 172, Loss: -7.31242e-01\n",
      "Iter 173, Loss: -7.32723e-01\n",
      "Iter 174, Loss: -7.30108e-01\n",
      "Iter 175, Loss: -7.32060e-01\n",
      "Iter 176, Loss: -7.32786e-01\n",
      "Iter 177, Loss: -7.35043e-01\n",
      "Iter 178, Loss: -7.33074e-01\n",
      "Iter 179, Loss: -7.34295e-01\n",
      "Iter 180, Loss: -7.33170e-01\n",
      "Iter 181, Loss: -7.33980e-01\n",
      "Iter 182, Loss: -7.35642e-01\n",
      "Iter 183, Loss: -7.37643e-01\n",
      "Iter 184, Loss: -7.36317e-01\n",
      "Iter 185, Loss: -7.32256e-01\n",
      "Iter 186, Loss: -7.36593e-01\n",
      "Iter 187, Loss: -7.31469e-01\n",
      "Iter 188, Loss: -7.29629e-01\n",
      "Iter 189, Loss: -7.28597e-01\n",
      "Iter 190, Loss: -7.28779e-01\n",
      "Iter 191, Loss: -7.27845e-01\n",
      "Iter 192, Loss: -7.32303e-01\n",
      "Iter 193, Loss: -7.30758e-01\n",
      "Iter 194, Loss: -7.33488e-01\n",
      "Iter 195, Loss: -7.34601e-01\n",
      "Iter 196, Loss: -7.39891e-01\n",
      "Iter 197, Loss: -7.38080e-01\n",
      "Iter 198, Loss: -7.33541e-01\n",
      "Iter 199, Loss: -7.33723e-01\n",
      "Iter 200, Loss: -7.32856e-01\n",
      "Iter 201, Loss: -7.31785e-01\n",
      "Iter 202, Loss: -7.28319e-01\n",
      "Iter 203, Loss: -7.26441e-01\n",
      "Iter 204, Loss: -7.26921e-01\n",
      "Iter 205, Loss: -7.23532e-01\n",
      "Iter 206, Loss: -7.21171e-01\n",
      "Iter 207, Loss: -7.24461e-01\n",
      "Iter 208, Loss: -7.27086e-01\n",
      "Iter 209, Loss: -7.25501e-01\n",
      "Iter 210, Loss: -7.24992e-01\n",
      "Iter 211, Loss: -7.22712e-01\n",
      "Iter 212, Loss: -7.23013e-01\n",
      "Iter 213, Loss: -7.18972e-01\n",
      "Iter 214, Loss: -7.16355e-01\n",
      "Iter 215, Loss: -7.16601e-01\n",
      "Iter 216, Loss: -7.21595e-01\n",
      "Iter 217, Loss: -7.21075e-01\n",
      "Iter 218, Loss: -7.26874e-01\n",
      "Iter 219, Loss: -7.25808e-01\n",
      "Iter 220, Loss: -7.30894e-01\n",
      "Iter 221, Loss: -7.31325e-01\n",
      "Iter 222, Loss: -7.27031e-01\n",
      "Iter 223, Loss: -7.22892e-01\n",
      "Iter 224, Loss: -7.21406e-01\n",
      "Iter 225, Loss: -7.20542e-01\n",
      "Iter 226, Loss: -7.21491e-01\n",
      "Iter 227, Loss: -7.21327e-01\n",
      "Iter 228, Loss: -7.20009e-01\n",
      "Iter 229, Loss: -7.19875e-01\n",
      "Iter 230, Loss: -7.18340e-01\n",
      "Iter 231, Loss: -7.19549e-01\n",
      "Iter 232, Loss: -7.25027e-01\n",
      "Iter 233, Loss: -7.22973e-01\n",
      "Iter 234, Loss: -7.21816e-01\n",
      "Iter 235, Loss: -7.20924e-01\n",
      "Iter 236, Loss: -7.21844e-01\n",
      "Iter 237, Loss: -7.21445e-01\n",
      "Iter 238, Loss: -7.19759e-01\n",
      "Iter 239, Loss: -7.19945e-01\n",
      "Iter 240, Loss: -7.24053e-01\n",
      "Iter 241, Loss: -7.27381e-01\n",
      "Iter 242, Loss: -7.25251e-01\n",
      "Iter 243, Loss: -7.24610e-01\n",
      "Iter 244, Loss: -7.26852e-01\n",
      "Iter 245, Loss: -7.28922e-01\n",
      "Iter 246, Loss: -7.27325e-01\n",
      "Iter 247, Loss: -7.23016e-01\n",
      "Iter 248, Loss: -7.23200e-01\n",
      "Iter 249, Loss: -7.29345e-01\n",
      "Iter 250, Loss: -7.32032e-01\n",
      "Iter 251, Loss: -7.30720e-01\n",
      "Iter 252, Loss: -7.34467e-01\n",
      "Iter 253, Loss: -7.34728e-01\n",
      "Iter 254, Loss: -7.35026e-01\n",
      "Iter 255, Loss: -7.36541e-01\n",
      "Iter 256, Loss: -7.33248e-01\n",
      "Iter 257, Loss: -7.35705e-01\n",
      "Iter 258, Loss: -7.36863e-01\n",
      "Iter 259, Loss: -7.44303e-01\n",
      "Iter 260, Loss: -7.45830e-01\n",
      "Iter 261, Loss: -7.47147e-01\n",
      "Iter 262, Loss: -7.52811e-01\n",
      "Iter 263, Loss: -7.54351e-01\n",
      "Iter 264, Loss: -7.57474e-01\n",
      "Iter 265, Loss: -7.61539e-01\n",
      "Iter 266, Loss: -7.61150e-01\n",
      "Iter 267, Loss: -7.61997e-01\n",
      "Iter 268, Loss: -7.61050e-01\n",
      "Iter 269, Loss: -7.53877e-01\n",
      "Iter 270, Loss: -7.57546e-01\n",
      "Iter 271, Loss: -7.51883e-01\n",
      "Iter 272, Loss: -7.55121e-01\n",
      "Iter 273, Loss: -7.56507e-01\n",
      "Iter 274, Loss: -7.57134e-01\n",
      "Iter 275, Loss: -7.63931e-01\n",
      "Iter 276, Loss: -7.59953e-01\n",
      "Iter 277, Loss: -7.61795e-01\n",
      "Iter 278, Loss: -7.62830e-01\n",
      "Iter 279, Loss: -7.63863e-01\n",
      "Iter 280, Loss: -7.64122e-01\n",
      "Iter 281, Loss: -7.57738e-01\n",
      "Iter 282, Loss: -7.56684e-01\n",
      "Iter 283, Loss: -7.57458e-01\n",
      "Iter 284, Loss: -7.55432e-01\n",
      "Iter 285, Loss: -7.54349e-01\n",
      "Iter 286, Loss: -7.54405e-01\n",
      "Iter 287, Loss: -7.51622e-01\n",
      "Iter 288, Loss: -7.51885e-01\n",
      "Iter 289, Loss: -7.54025e-01\n",
      "Iter 290, Loss: -7.56830e-01\n",
      "Iter 291, Loss: -7.57156e-01\n",
      "Iter 292, Loss: -7.60701e-01\n",
      "Iter 293, Loss: -7.64266e-01\n",
      "Iter 294, Loss: -7.58323e-01\n",
      "Iter 295, Loss: -7.58406e-01\n",
      "Iter 296, Loss: -7.62067e-01\n",
      "Iter 297, Loss: -7.62654e-01\n",
      "Iter 298, Loss: -7.60130e-01\n",
      "Iter 299, Loss: -7.53141e-01\n",
      "Iter 300, Loss: -7.51751e-01\n",
      "Iter 301, Loss: -7.48515e-01\n",
      "Iter 302, Loss: -7.45928e-01\n",
      "Iter 303, Loss: -7.45207e-01\n",
      "Iter 304, Loss: -7.42368e-01\n",
      "Iter 305, Loss: -7.42901e-01\n",
      "Iter 306, Loss: -7.46863e-01\n",
      "Iter 307, Loss: -7.42164e-01\n",
      "Iter 308, Loss: -7.37441e-01\n",
      "Iter 309, Loss: -7.39075e-01\n",
      "Iter 310, Loss: -7.35543e-01\n",
      "Iter 311, Loss: -7.37348e-01\n",
      "Iter 312, Loss: -7.39797e-01\n",
      "Iter 313, Loss: -7.36377e-01\n",
      "Iter 314, Loss: -7.34306e-01\n",
      "Iter 315, Loss: -7.35947e-01\n",
      "Iter 316, Loss: -7.39918e-01\n",
      "Iter 317, Loss: -7.43615e-01\n",
      "Iter 318, Loss: -7.44606e-01\n",
      "Iter 319, Loss: -7.39097e-01\n",
      "Iter 320, Loss: -7.40607e-01\n",
      "Iter 321, Loss: -7.37331e-01\n",
      "Iter 322, Loss: -7.37033e-01\n",
      "Iter 323, Loss: -7.34795e-01\n",
      "Iter 324, Loss: -7.43548e-01\n",
      "Iter 325, Loss: -7.42312e-01\n",
      "Iter 326, Loss: -7.40339e-01\n",
      "Iter 327, Loss: -7.41255e-01\n",
      "Iter 328, Loss: -7.38560e-01\n",
      "Iter 329, Loss: -7.36415e-01\n",
      "Iter 330, Loss: -7.37981e-01\n",
      "Iter 331, Loss: -7.41338e-01\n",
      "Iter 332, Loss: -7.46331e-01\n",
      "Iter 333, Loss: -7.41103e-01\n",
      "Iter 334, Loss: -7.38394e-01\n",
      "Iter 335, Loss: -7.41281e-01\n",
      "Iter 336, Loss: -7.32743e-01\n",
      "Iter 337, Loss: -7.35086e-01\n",
      "Iter 338, Loss: -7.34591e-01\n",
      "Iter 339, Loss: -7.40072e-01\n",
      "Iter 340, Loss: -7.43741e-01\n",
      "Iter 341, Loss: -7.43098e-01\n",
      "Iter 342, Loss: -7.43164e-01\n",
      "Iter 343, Loss: -7.39483e-01\n",
      "Iter 344, Loss: -7.40090e-01\n",
      "Iter 345, Loss: -7.43491e-01\n",
      "Iter 346, Loss: -7.42765e-01\n",
      "Iter 347, Loss: -7.38788e-01\n",
      "Iter 348, Loss: -7.38344e-01\n",
      "Iter 349, Loss: -7.41384e-01\n",
      "Iter 350, Loss: -7.44161e-01\n",
      "Iter 351, Loss: -7.42641e-01\n",
      "Iter 352, Loss: -7.44607e-01\n",
      "Iter 353, Loss: -7.40998e-01\n",
      "Iter 354, Loss: -7.39116e-01\n",
      "Iter 355, Loss: -7.43865e-01\n",
      "Iter 356, Loss: -7.41871e-01\n",
      "Iter 357, Loss: -7.43808e-01\n",
      "Iter 358, Loss: -7.38329e-01\n",
      "Iter 359, Loss: -7.37536e-01\n",
      "Iter 360, Loss: -7.32453e-01\n",
      "Iter 361, Loss: -7.30679e-01\n",
      "Iter 362, Loss: -7.34093e-01\n",
      "Iter 363, Loss: -7.33110e-01\n",
      "Iter 364, Loss: -7.35995e-01\n",
      "Iter 365, Loss: -7.38862e-01\n",
      "Iter 366, Loss: -7.37599e-01\n",
      "Iter 367, Loss: -7.35621e-01\n",
      "Iter 368, Loss: -7.34624e-01\n",
      "Iter 369, Loss: -7.33717e-01\n",
      "Iter 370, Loss: -7.24461e-01\n",
      "Iter 371, Loss: -7.22404e-01\n",
      "Iter 372, Loss: -7.25558e-01\n",
      "Iter 373, Loss: -7.21200e-01\n",
      "Iter 374, Loss: -7.19850e-01\n",
      "Iter 375, Loss: -7.20351e-01\n",
      "Iter 376, Loss: -7.18570e-01\n",
      "Iter 377, Loss: -7.10827e-01\n",
      "Iter 378, Loss: -7.05824e-01\n",
      "Iter 379, Loss: -7.03507e-01\n",
      "Iter 380, Loss: -6.90988e-01\n",
      "Iter 381, Loss: -6.89963e-01\n",
      "Iter 382, Loss: -6.83300e-01\n",
      "Iter 383, Loss: -6.78674e-01\n",
      "Iter 384, Loss: -6.78339e-01\n",
      "Iter 385, Loss: -6.74000e-01\n",
      "Iter 386, Loss: -6.68866e-01\n",
      "Iter 387, Loss: -6.66066e-01\n",
      "Iter 388, Loss: -6.60129e-01\n",
      "Iter 389, Loss: -6.57036e-01\n",
      "Iter 390, Loss: -6.49044e-01\n",
      "Iter 391, Loss: -6.43887e-01\n",
      "Iter 392, Loss: -6.38955e-01\n",
      "Iter 393, Loss: -6.33756e-01\n",
      "Iter 394, Loss: -6.30057e-01\n",
      "Iter 395, Loss: -6.23082e-01\n",
      "Iter 396, Loss: -6.15836e-01\n",
      "Iter 397, Loss: -6.14313e-01\n",
      "Iter 398, Loss: -6.09630e-01\n",
      "Iter 399, Loss: -6.09341e-01\n",
      "Iter 400, Loss: -6.02439e-01\n",
      "Iter 401, Loss: -5.99862e-01\n",
      "Iter 402, Loss: -6.02371e-01\n",
      "Iter 403, Loss: -6.02956e-01\n",
      "Iter 404, Loss: -6.03443e-01\n",
      "Iter 405, Loss: -6.01949e-01\n",
      "Iter 406, Loss: -5.98774e-01\n",
      "Iter 407, Loss: -5.87557e-01\n",
      "Iter 408, Loss: -5.88044e-01\n",
      "Iter 409, Loss: -5.85054e-01\n",
      "Iter 410, Loss: -5.84069e-01\n",
      "Iter 411, Loss: -5.82679e-01\n",
      "Iter 412, Loss: -5.82403e-01\n",
      "Iter 413, Loss: -5.84720e-01\n",
      "Iter 414, Loss: -5.83786e-01\n",
      "Iter 415, Loss: -5.81383e-01\n",
      "Iter 416, Loss: -5.81540e-01\n",
      "Iter 417, Loss: -5.79173e-01\n",
      "Iter 418, Loss: -5.75592e-01\n",
      "Iter 419, Loss: -5.73910e-01\n",
      "Iter 420, Loss: -5.67607e-01\n",
      "Iter 421, Loss: -5.63940e-01\n",
      "Iter 422, Loss: -5.59672e-01\n",
      "Iter 423, Loss: -5.50658e-01\n",
      "Iter 424, Loss: -5.46176e-01\n",
      "Iter 425, Loss: -5.46984e-01\n",
      "Iter 426, Loss: -5.41633e-01\n",
      "Iter 427, Loss: -5.42187e-01\n",
      "Iter 428, Loss: -5.41786e-01\n",
      "Iter 429, Loss: -5.46876e-01\n",
      "Iter 430, Loss: -5.45053e-01\n",
      "Iter 431, Loss: -5.39082e-01\n",
      "Iter 432, Loss: -5.37069e-01\n",
      "Iter 433, Loss: -5.35612e-01\n",
      "Iter 434, Loss: -5.35806e-01\n",
      "Iter 435, Loss: -5.36593e-01\n",
      "Iter 436, Loss: -5.31269e-01\n",
      "Iter 437, Loss: -5.31596e-01\n",
      "Iter 438, Loss: -5.31583e-01\n",
      "Iter 439, Loss: -5.25061e-01\n",
      "Iter 440, Loss: -5.27235e-01\n",
      "Iter 441, Loss: -5.24121e-01\n",
      "Iter 442, Loss: -5.19435e-01\n",
      "Iter 443, Loss: -5.16362e-01\n",
      "Iter 444, Loss: -5.13742e-01\n",
      "Iter 445, Loss: -5.13129e-01\n",
      "Iter 446, Loss: -5.07277e-01\n",
      "Iter 447, Loss: -5.02651e-01\n",
      "Iter 448, Loss: -5.03937e-01\n",
      "Iter 449, Loss: -5.05929e-01\n",
      "Iter 450, Loss: -5.04924e-01\n",
      "Iter 451, Loss: -5.05385e-01\n",
      "Iter 452, Loss: -5.07323e-01\n",
      "Iter 453, Loss: -5.08053e-01\n",
      "Iter 454, Loss: -5.08311e-01\n",
      "Iter 455, Loss: -5.12271e-01\n",
      "Iter 456, Loss: -5.15627e-01\n",
      "Iter 457, Loss: -5.13424e-01\n",
      "Iter 458, Loss: -5.14128e-01\n",
      "Iter 459, Loss: -5.12493e-01\n",
      "Iter 460, Loss: -5.15984e-01\n",
      "Iter 461, Loss: -5.14890e-01\n",
      "Iter 462, Loss: -5.17955e-01\n",
      "Iter 463, Loss: -5.22524e-01\n",
      "Iter 464, Loss: -5.27017e-01\n",
      "Iter 465, Loss: -5.30319e-01\n",
      "Iter 466, Loss: -5.35557e-01\n",
      "Iter 467, Loss: -5.36161e-01\n",
      "Iter 468, Loss: -5.39849e-01\n",
      "Iter 469, Loss: -5.39656e-01\n",
      "Iter 470, Loss: -5.41799e-01\n",
      "Iter 471, Loss: -5.42165e-01\n",
      "Iter 472, Loss: -5.47751e-01\n",
      "Iter 473, Loss: -5.50355e-01\n",
      "Iter 474, Loss: -5.51600e-01\n",
      "Iter 475, Loss: -5.50265e-01\n",
      "Iter 476, Loss: -5.51059e-01\n",
      "Iter 477, Loss: -5.55324e-01\n",
      "Iter 478, Loss: -5.58344e-01\n",
      "Iter 479, Loss: -5.63932e-01\n",
      "Iter 480, Loss: -5.62632e-01\n",
      "Iter 481, Loss: -5.65192e-01\n",
      "Iter 482, Loss: -5.66326e-01\n",
      "Iter 483, Loss: -5.71697e-01\n",
      "Iter 484, Loss: -5.75561e-01\n",
      "Iter 485, Loss: -5.74958e-01\n",
      "Iter 486, Loss: -5.77302e-01\n",
      "Iter 487, Loss: -5.80071e-01\n",
      "Iter 488, Loss: -5.83187e-01\n",
      "Iter 489, Loss: -5.85010e-01\n",
      "Iter 490, Loss: -5.86009e-01\n",
      "Iter 491, Loss: -5.86306e-01\n",
      "Iter 492, Loss: -5.86328e-01\n",
      "Iter 493, Loss: -5.87331e-01\n",
      "Iter 494, Loss: -5.87961e-01\n",
      "Iter 495, Loss: -5.87254e-01\n",
      "Iter 496, Loss: -5.87853e-01\n",
      "Iter 497, Loss: -5.91338e-01\n",
      "Iter 498, Loss: -5.92689e-01\n",
      "Iter 499, Loss: -5.94376e-01\n",
      "Iter 500, Loss: -5.93722e-01\n",
      "Iter 501, Loss: -5.96303e-01\n",
      "Iter 502, Loss: -5.92538e-01\n",
      "Iter 503, Loss: -5.90684e-01\n",
      "Iter 504, Loss: -5.91472e-01\n",
      "Iter 505, Loss: -5.94391e-01\n",
      "Iter 506, Loss: -5.97164e-01\n",
      "Iter 507, Loss: -5.98218e-01\n",
      "Iter 508, Loss: -6.00047e-01\n",
      "Iter 509, Loss: -6.03872e-01\n",
      "Iter 510, Loss: -6.05441e-01\n",
      "Iter 511, Loss: -6.00364e-01\n",
      "Iter 512, Loss: -5.99644e-01\n",
      "Iter 513, Loss: -5.98917e-01\n",
      "Iter 514, Loss: -5.96403e-01\n",
      "Iter 515, Loss: -5.92923e-01\n",
      "Iter 516, Loss: -5.90022e-01\n",
      "Iter 517, Loss: -5.89118e-01\n",
      "Iter 518, Loss: -5.90437e-01\n",
      "Iter 519, Loss: -5.90117e-01\n",
      "Iter 520, Loss: -5.89774e-01\n",
      "Iter 521, Loss: -5.86811e-01\n",
      "Iter 522, Loss: -5.80323e-01\n",
      "Iter 523, Loss: -5.75633e-01\n",
      "Iter 524, Loss: -5.70712e-01\n",
      "Iter 525, Loss: -5.69296e-01\n",
      "Iter 526, Loss: -5.68274e-01\n",
      "Iter 527, Loss: -5.63226e-01\n",
      "Iter 528, Loss: -5.57206e-01\n",
      "Iter 529, Loss: -5.57693e-01\n",
      "Iter 530, Loss: -5.50383e-01\n",
      "Iter 531, Loss: -5.45023e-01\n",
      "Iter 532, Loss: -5.41376e-01\n",
      "Iter 533, Loss: -5.42967e-01\n",
      "Iter 534, Loss: -5.41494e-01\n",
      "Iter 535, Loss: -5.39869e-01\n",
      "Iter 536, Loss: -5.35750e-01\n",
      "Iter 537, Loss: -5.36629e-01\n",
      "Iter 538, Loss: -5.32214e-01\n",
      "Iter 539, Loss: -5.31700e-01\n",
      "Iter 540, Loss: -5.28214e-01\n",
      "Iter 541, Loss: -5.27079e-01\n",
      "Iter 542, Loss: -5.25027e-01\n",
      "Iter 543, Loss: -5.24021e-01\n",
      "Iter 544, Loss: -5.25136e-01\n",
      "Iter 545, Loss: -5.24039e-01\n",
      "Iter 546, Loss: -5.25093e-01\n",
      "Iter 547, Loss: -5.26569e-01\n",
      "Iter 548, Loss: -5.29241e-01\n",
      "Iter 549, Loss: -5.26190e-01\n",
      "Iter 550, Loss: -5.27997e-01\n",
      "Iter 551, Loss: -5.28987e-01\n",
      "Iter 552, Loss: -5.28628e-01\n",
      "Iter 553, Loss: -5.26133e-01\n",
      "Iter 554, Loss: -5.23150e-01\n",
      "Iter 555, Loss: -5.20683e-01\n",
      "Iter 556, Loss: -5.19477e-01\n",
      "Iter 557, Loss: -5.14472e-01\n",
      "Iter 558, Loss: -5.16226e-01\n",
      "Iter 559, Loss: -5.11901e-01\n",
      "Iter 560, Loss: -5.09940e-01\n",
      "Iter 561, Loss: -5.12328e-01\n",
      "Iter 562, Loss: -5.14172e-01\n",
      "Iter 563, Loss: -5.10793e-01\n",
      "Iter 564, Loss: -5.15788e-01\n",
      "Iter 565, Loss: -5.17312e-01\n",
      "Iter 566, Loss: -5.20639e-01\n",
      "Iter 567, Loss: -5.24052e-01\n",
      "Iter 568, Loss: -5.23053e-01\n",
      "Iter 569, Loss: -5.22028e-01\n",
      "Iter 570, Loss: -5.23101e-01\n",
      "Iter 571, Loss: -5.22158e-01\n",
      "Iter 572, Loss: -5.23308e-01\n",
      "Iter 573, Loss: -5.19568e-01\n",
      "Iter 574, Loss: -5.18005e-01\n",
      "Iter 575, Loss: -5.17643e-01\n",
      "Iter 576, Loss: -5.11898e-01\n",
      "Iter 577, Loss: -5.07059e-01\n",
      "Iter 578, Loss: -5.00547e-01\n",
      "Iter 579, Loss: -5.01003e-01\n",
      "Iter 580, Loss: -4.98510e-01\n",
      "Iter 581, Loss: -4.95994e-01\n",
      "Iter 582, Loss: -4.93810e-01\n",
      "Iter 583, Loss: -4.88265e-01\n",
      "Iter 584, Loss: -4.84228e-01\n",
      "Iter 585, Loss: -4.82208e-01\n",
      "Iter 586, Loss: -4.85896e-01\n",
      "Iter 587, Loss: -4.82008e-01\n",
      "Iter 588, Loss: -4.79658e-01\n",
      "Iter 589, Loss: -4.79980e-01\n",
      "Iter 590, Loss: -4.77636e-01\n",
      "Iter 591, Loss: -4.71932e-01\n",
      "Iter 592, Loss: -4.71380e-01\n",
      "Iter 593, Loss: -4.69020e-01\n",
      "Iter 594, Loss: -4.64997e-01\n",
      "Iter 595, Loss: -4.62492e-01\n",
      "Iter 596, Loss: -4.59630e-01\n",
      "Iter 597, Loss: -4.57215e-01\n",
      "Iter 598, Loss: -4.50695e-01\n",
      "Iter 599, Loss: -4.48321e-01\n",
      "Iter 600, Loss: -4.48490e-01\n",
      "Iter 601, Loss: -4.45387e-01\n",
      "Iter 602, Loss: -4.43450e-01\n",
      "Iter 603, Loss: -4.39080e-01\n",
      "Iter 604, Loss: -4.38945e-01\n",
      "Iter 605, Loss: -4.36700e-01\n",
      "Iter 606, Loss: -4.32079e-01\n",
      "Iter 607, Loss: -4.27672e-01\n",
      "Iter 608, Loss: -4.27767e-01\n",
      "Iter 609, Loss: -4.18799e-01\n",
      "Iter 610, Loss: -4.15552e-01\n",
      "Iter 611, Loss: -4.16090e-01\n",
      "Iter 612, Loss: -4.16860e-01\n",
      "Iter 613, Loss: -4.16469e-01\n",
      "Iter 614, Loss: -4.16779e-01\n",
      "Iter 615, Loss: -4.18240e-01\n",
      "Iter 616, Loss: -4.13218e-01\n",
      "Iter 617, Loss: -4.11061e-01\n",
      "Iter 618, Loss: -4.11414e-01\n",
      "Iter 619, Loss: -4.05655e-01\n",
      "Iter 620, Loss: -4.04427e-01\n",
      "Iter 621, Loss: -4.00283e-01\n",
      "Iter 622, Loss: -3.97143e-01\n",
      "Iter 623, Loss: -3.97492e-01\n",
      "Iter 624, Loss: -3.96323e-01\n",
      "Iter 625, Loss: -3.93236e-01\n",
      "Iter 626, Loss: -3.93351e-01\n",
      "Iter 627, Loss: -3.89700e-01\n",
      "Iter 628, Loss: -3.86203e-01\n",
      "Iter 629, Loss: -3.84659e-01\n",
      "Iter 630, Loss: -3.84858e-01\n",
      "Iter 631, Loss: -3.77083e-01\n",
      "Iter 632, Loss: -3.78862e-01\n",
      "Iter 633, Loss: -3.72622e-01\n",
      "Iter 634, Loss: -3.69971e-01\n",
      "Iter 635, Loss: -3.71005e-01\n",
      "Iter 636, Loss: -3.67600e-01\n",
      "Iter 637, Loss: -3.61238e-01\n",
      "Iter 638, Loss: -3.60651e-01\n",
      "Iter 639, Loss: -3.63221e-01\n",
      "Iter 640, Loss: -3.62359e-01\n",
      "Iter 641, Loss: -3.63328e-01\n",
      "Iter 642, Loss: -3.61005e-01\n",
      "Iter 643, Loss: -3.57955e-01\n",
      "Iter 644, Loss: -3.60443e-01\n",
      "Iter 645, Loss: -3.62400e-01\n",
      "Iter 646, Loss: -3.64434e-01\n",
      "Iter 647, Loss: -3.66610e-01\n",
      "Iter 648, Loss: -3.63931e-01\n",
      "Iter 649, Loss: -3.62946e-01\n",
      "Iter 650, Loss: -3.61651e-01\n",
      "Iter 651, Loss: -3.64233e-01\n",
      "Iter 652, Loss: -3.63153e-01\n",
      "Iter 653, Loss: -3.59940e-01\n",
      "Iter 654, Loss: -3.59228e-01\n",
      "Iter 655, Loss: -3.59259e-01\n",
      "Iter 656, Loss: -3.59983e-01\n",
      "Iter 657, Loss: -3.62377e-01\n",
      "Iter 658, Loss: -3.65765e-01\n",
      "Iter 659, Loss: -3.65565e-01\n",
      "Iter 660, Loss: -3.65099e-01\n",
      "Iter 661, Loss: -3.64025e-01\n",
      "Iter 662, Loss: -3.65717e-01\n",
      "Iter 663, Loss: -3.64987e-01\n",
      "Iter 664, Loss: -3.64726e-01\n",
      "Iter 665, Loss: -3.65636e-01\n",
      "Iter 666, Loss: -3.64231e-01\n",
      "Iter 667, Loss: -3.66148e-01\n",
      "Iter 668, Loss: -3.64629e-01\n",
      "Iter 669, Loss: -3.67032e-01\n",
      "Iter 670, Loss: -3.62839e-01\n",
      "Iter 671, Loss: -3.64483e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m, in \u001b[0;36mScore_Matching.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\optim\\asgd.py:74\u001b[0m, in \u001b[0;36mASGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m---> 74\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m     77\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mScore_Matching.loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(v)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnex):\n\u001b[1;32m---> 31\u001b[0m     u_v \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     div_u \u001b[38;5;241m=\u001b[39m div_u \u001b[38;5;241m+\u001b[39m u_v\u001b[38;5;241m.\u001b[39mtrace()\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnorm(u)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mdiv_u ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnex\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\autograd\\functional.py:578\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    575\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 578\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(outputs,\n\u001b[0;32m    580\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    581\u001b[0m                                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    582\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mDNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnTh\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     25\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i](x))\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_cuda117\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u = model.predict().cpu().detach().numpy()\n",
    "v1 = v.cpu().detach().numpy()\n",
    "plt.plot(v1, u, 'o')\n",
    "plt.plot(v1,-1*v1,'-')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(v):\n",
    "    x=v**2 + 3*v[0] + v[1]\n",
    "    return x\n",
    "nex = 3\n",
    "d = 2\n",
    "v = torch.ones(nex, d, requires_grad=True)\n",
    "for i in range(nex):\n",
    "    a = torch.autograd.functional.jacobian(func, v[i,:], create_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
